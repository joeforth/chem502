{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workshop 3 - ML Predictions of Aqueous Solubility\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this workshop, you will get some hands on practice of applying some of the major machine learning (ML) models to a chemical dataset.\n",
    "\n",
    "### The data\n",
    "\n",
    "AqSolDB ([Sorkun et al.](https://doi.org/10.1038/s41597-019-0151-1)) is a curated dataset of experimentally-determined aqueous solubility values, with calculated descriptors for the molecules.\n",
    "\n",
    "The paper gives details on how the data was acquired and processed, and its availability on a number of platforms including [github](https://github.com/mcsorkun/AqSolDB)\n",
    "\n",
    "\n",
    "### The task\n",
    "\n",
    "Prepare the data for training and evaulating a set of machine learning models to predict the solubilty of the compounds based on the features supplied (and others if you would like to calculate additional descriptors as features).\n",
    "\n",
    "You will use scikit-learn to train and evaluate the following Supervised Learning models:\n",
    "\n",
    "- Linear regression\n",
    "- Logistic regression\n",
    "- k-Nearest neighbors\n",
    "\n",
    "### Steps\n",
    "\n",
    "1. Load the data\n",
    "2. Perform some EDA to gain initial understanding of the distribution of features and relationships between features, and with the target.\n",
    "\n",
    "For each model (may require additional stages depending on the model)\n",
    "\n",
    "3. Prepare the data \n",
    "4. Train the model\n",
    "5. Make predictions\n",
    "6. Evaluate performance\n",
    "\n",
    "7. Analyse the performance of the models. Draw conclusions about the chemical problem, e.g. from the feature importances.\n",
    "\n",
    "\n",
    "### Some Possibly Useful Reading\n",
    "\n",
    "- The [scikit-learn documentation](https://scikit-learn.org/stable/) contains a huge range of examples\n",
    "- The [Hundred-Page Machine Learning Book](https://themlbook.com/wiki/doku.php \n",
    ") goes through the ML methods we use here exceptionally clearly.\n",
    "- The Course Book contains both a [Chapter](https://joeforth.github.io/chem502_book/ml-intro/ml-intro/) and a [Workbook](https://joeforth.github.io/chem502_book/ml-intro/ml-demo/) with examples relevant to this Workshop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Import Modules\n",
    "\n",
    "We'll be using a Python module called [scikit-learn](https://scikit-learn.org/stable/), it's a big complicated piece of software and we'll just install a few modules from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "from random import choice\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn import linear_model\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you do not have scikit-learn installed, uncomment the following line\n",
    "# !conda install -y -c conda-forge scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have issues with VSCode notebook cell outputs being truncated:\n",
    "\n",
    "- Go to Settings (via menubar or cmd-, on Mac)\n",
    "- Search for cell output settings: try @tag:notebookOutputLayout\n",
    "- Adjust settings, e.g. scrolling, number of lines to display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Clean the Data\n",
    "\n",
    "First, perform some initial exploratory analysis of the dataset using some of the methods you've used in previous workshops.\n",
    "\n",
    "In addition to looking for distribution and patterns in the data, look at what the columns actually contain. Some will include metadata about the source of the observation and its processing, which will not be relevant to the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: \n",
    "# 1. Check the data and load into a DataFrame\n",
    "# 2. Check the data types\n",
    "# 3. Check for missing values\n",
    "# 4. Check summary statistics\n",
    "# 5. Identify and drop redundant columns (e.g., those containing experiment metadata)\n",
    "# 6. Identify and convert columns that should store data in a more appropriate format (e.g., int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Perform Exploratory Data Analysis\n",
    "\n",
    "Now get a feel for your dataset - perform exploratory data analysis to see what correlations and outliers there are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# 1. Visualise the data to look for distributions of features, check for outliers\n",
    "# 2. Visualise the data to look for correlations\n",
    "# 3. Visualise the data to look for relationships between features and target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions for Part 2\n",
    "\n",
    "In your submission - add a block of markdown discussing each of these.\n",
    "\n",
    "- Explain your approach to EDA for the dataset. What questions can this process answer and suggest how it can aid the subsequent analysis and modelling.\n",
    "- What are the most significant correlations in this dataset? Discuss any strong relationships between the features and the target variable that are apparent.\n",
    "- If you were selecting features from the data, are there any that you would remove? Explain why/why not.\n",
    "\n",
    "\n",
    "#### Things to consider\n",
    "\n",
    "- Note the distributions of values of features (e.g. the measures of centre, the magnitude and shape of the distribution and the range of the values).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Linear regression\n",
    "\n",
    "The first model we will apply is a [linear regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html).\n",
    "\n",
    "Linear regression models the relationship between input features and a continuous target variable using a linear function.\n",
    "\n",
    "The function looks like:  \n",
    "\n",
    "$$\n",
    "y = w_0 + w_1x_1 + w_2x_2 + \\dots + w_nx_n + \\epsilon\n",
    "$$\n",
    "\n",
    "Where:  \n",
    "- $ y $ = **Predicted output** (target variable)  \n",
    "- $ x_1, x_2, \\dots, x_n $ = **Input features** (independent variables)  \n",
    "- $ w_0 $ = **Intercept** (bias term)  \n",
    "- $ w_1, w_2, \\dots, w_n $ = **Coefficients** (weights)  \n",
    "- $ \\epsilon $ = **Error term** (accounts for noise in data)  \n",
    "\n",
    "The goal is to find weights $w_{i}$ that minimize the error, typically using Ordinary Least Squares (OLS).\n",
    "\n",
    "It finds a best-fit line by minimising the difference between predictions and actual values, typically using least squares. \n",
    "\n",
    "It is widely used for trend analysis, forecasting, and understanding feature impact on outcomes.\n",
    "\n",
    "You may find the contents of the [Notebook on Machine Learning](https://joeforth.github.io/chem502_book/ml-intro/ml-demo/) in the course book useful here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Prepare data\n",
    "\n",
    "To prepare the data, create a new dataframe containing only the numerical features of the AqSolDB dataset. Then, separate your data into the features (the predictor variables) and target (the variable you want to predict)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# 1 - Read the target column into a separate variable\n",
    "# 2 - Read the feature columns into a different variable - remember to drop the target column\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Create the training and test sets\n",
    "\n",
    "Run [`train_test_split`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) to create separate training and test sets, with 20% of the samples in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7985, 17), (1997, 17), (7985,), (1997,))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO \n",
    "# \n",
    "# 3 - Split the data into training (80%) and testing (20%) sets and check the size of the resulting datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Training the model\n",
    "\n",
    "It is time to train the first ML model.\n",
    "\n",
    "You will need to create a new LinearRegression model and train it using its `fit` method on the training data's features.\n",
    "\n",
    "The [scikit-learn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) on Linear Regression may be useful here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 4 - Create a linear regression model\n",
    "\n",
    "# TODO: 5 - Fit the model to the training data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Test the model's performance on unseen data\n",
    "\n",
    "You can now get the model to predict the solubilities for the subset of data you withheld for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 6 - Predict the solubility of the test set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Evaluating the model's performance\n",
    "\n",
    "We can visualise how closely the predicted solubility values for both the training and/or test set match the real values.\n",
    "\n",
    "***\n",
    "Hint - You will need to also generate predictions for the test set if you want to visualise\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a variety of metrics that can be used to quantify the model's performance. \n",
    "\n",
    "One commonly used metric for regression tasks is $r^2$ which expresses how well the model fits the data. It ranges from 0 to 1, with 1 indicating a perfect fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: \n",
    "# \n",
    "# 7 - Calculate r^2 value of how well your model fits the data\n",
    "# 8 - Plot the predicted vs. actual values for the training set\n",
    "# 9 - Plot the predicted vs. actual values for the test set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions for Part 3\n",
    "\n",
    "In your submission - add a block of markdown discussing each of these.\n",
    "\n",
    "- What other metrics might be useful for evaluating the model's performance? Choose one other metric and calculate it for the model's perform on the test data. Briefly explain the form and meaning of the metric.\n",
    "- Comment on the performance of the model on the training vs. the test data. Is there anything you can infer from the comparison?\n",
    "- What information can you gain from the model coefficients? \n",
    "\n",
    "The [Book Section on ML](https://joeforth.github.io/chem502_book/ml-intro/ml-intro/) as well as the [Introduction to ML Notebook](https://joeforth.github.io/chem502_book/ml-intro/ml-demo/) may be helpful here. \n",
    "\n",
    "How could you use this information to improve model or the training process?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Logistic Regression\n",
    "\n",
    "[Logistic regression](https://www.geeksforgeeks.org/understanding-logistic-regression/) is used for binary classification: predict to which of two classes an input belongs. Scikit-learn's [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) for info on its `LogisticRegression` classifier. The python you write is very similar to what you have seen for the linear regression task.\n",
    "\n",
    "In the context of AqSolDB, we can convert solubility values (logS) into two classes:\n",
    "\n",
    "Soluble (1): logS above a certain threshold (e.g., logS > -2)\n",
    "Insoluble (0): logS below the threshold\n",
    "\n",
    "This allows us to predict solubility as a classification problem.\n",
    "\n",
    "### 4.1 Prepare data\n",
    "\n",
    "Get a copy of the dataframe after you had dropped the non-numeric features.\n",
    "\n",
    "You will need to add a new target variable based on the current `solubility` column, where the new column value is:\n",
    "\n",
    "`1` if `logS >= -2`  \n",
    "\n",
    "`0` if `logS < -2`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: \n",
    "# 1. Create a copy of the original DataFrame with numeric columns only\n",
    "# 2. Add a new column with binary solubility values\n",
    "# 3. Drop the original solubility column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Separate features and target and test-train split\n",
    "\n",
    "Follow the same process as for the linear regression and separate the target and feature columns.\n",
    "\n",
    "Then split the data into training and testing sets. Make sure you run this with `stratify=<name of your target array>`. (What does [`stratify`](https://machinelearningmastery.com/train-test-split-for-evaluating-machine-learning-algorithms#:~:text=stratified%20train-test%20split) do?)\n",
    "\n",
    "Both Logistic Regression and the next model (K-Nearest Neighbours) are classification algorithms, you need to standardise your data before training your models - for this Workshop, it's recommended to use the [scikit-learn's StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html).\n",
    "\n",
    "Some example code showing you how to do this is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_full, y_full, test_size=0.2, random_state=42, stratify=y_full)\n",
    "\n",
    "# Create a scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler to the training data\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# ransform the training data\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "\n",
    "# Alternatively you can fit the scaler and transform the training data (steps 3+4) at once\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Transform the testing data\n",
    "X_test_scaled = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# 4. Separate features and target column\n",
    "# 5. Split the data into training and testing sets - split first\n",
    "# 6. Scale the features using StandardScaler - scale the test and training sets separately\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: \n",
    "# 7. Create a logistic regression model\n",
    "# 8. Fit the model to the training data\n",
    "# 9. Predict the solubility of the test set\n",
    "# 10. Calculate the accuracy of the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [`classification report`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html) provides a set of metrics for classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions for Part 4\n",
    "\n",
    "In your submission - add a block of markdown discussing each of these.\n",
    "\n",
    "- Briefly explain the meaning of the metrics in the classification report.\n",
    "- Comment on the performance of the regression and classification models. Why might this approach be useful for some types of problems?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. k-NN classification\n",
    "\n",
    "Over to you for this one. Here is the documentation for sklearn's [`KNeighboursClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html).\n",
    "\n",
    "The process follows a very similar process to the models you have already seen.\n",
    "\n",
    "You can either stick with the binary classification or use the classes described in the AqSolDB paper:\n",
    "\n",
    "\n",
    "| Category | logS range |\n",
    "|------------|----------|\n",
    "|**Highly soluble** | logS > 0 |\n",
    "|**Soluble** | 0 > logS > -2 |\n",
    "|**Slightly soluble**  | -2 > logS > -4 |\n",
    "|**Insoluble** | logS < -4 |\n",
    "\n",
    "\n",
    "There are a few important points:\n",
    "\n",
    "\n",
    "1. Make sure you use `stratify` when you split the data and pass it the full target array.\n",
    "2. You must scale the features using StandardScaler after splitting.\n",
    "3. k-NN has a hyperparameter, so you will need to use cross-validation to adjust the value of k. There is a quick tutorial [here](https://www.datacamp.com/tutorial/k-nearest-neighbor-classification-scikit-learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Devise your evaluation strategy for the k-NN model\n",
    "\n",
    "Scikit-learn has a variety of methods to [measure and present](https://scikit-learn.org/stable/api/sklearn.metrics.html) model performance, e.g.\n",
    "\n",
    "- classification report \n",
    "- confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions for Part 5\n",
    "\n",
    "In your submission - add a block of markdown discussing each of these.\n",
    "\n",
    "- Based on the models you have trained and tested, how would you decide which model is most appropriate for predicting solubility categories? Consider the evaluation metrics, feature selection, and any limitations you observed.\n",
    "\n",
    "- Suggest one way that you could you iteratively refine your approach - e.g. adjusting models, features, or preprocessing steps - to improve predictive performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Summary\n",
    "\n",
    "As you have worked through this notebook, you have \n",
    "\n",
    "- Used exploratory analysis to identify and understand the structure of and trends within a moderately-sized chemical dataset.\n",
    "\n",
    "- Prepared a dataset to apply predictive modelling.\n",
    "\n",
    "- Trained, tested and evaluated some frequently-used ML models to predict a chemical property.\n",
    "\n",
    "In addition to the practical and technical skills you will have acquired in applying machine learning for this task, as part of the process, you have seen that it is important to critically consider how best to use the data you have available to address the scientific problem that you have.\n",
    "\n",
    "The process of structuring your data, selecting a model, selecting features, etc. can be a highly iterative process. It is important to think critically about how your data is being processed, how the model is learning from it, and how well the model’s predictions align with the real-world problem you are addressing.\n",
    "\n",
    "Machine learning is not a black-box tool but a structured approach that requires careful decision-making at every stage. This includes selecting appropriate features, choosing a suitable model, and ensuring rigorous evaluation of performance. Model results should not be taken at face value: It is essential to assess accuracy, biases, and generalisation.\n",
    "\n",
    "By approaching ML critically and iteratively, you can refine your models, improve predictions, and ensure that the insights gained are scientifically meaningful and reliable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "admissions",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
